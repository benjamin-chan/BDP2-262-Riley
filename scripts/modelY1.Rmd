# Model 1

Prediction model for `Y1`.


## Data preprocessing

Split data set into 60:40 training:validation samples.

```{r}
inTrain <- createDataPartition(df$id, p = 0.7)
dfTrain <- df[inTrain$Resample1, ]
dfValid <- df[-inTrain$Resample1, ]
```

Preprocess the training sample.

1. Exclude near-zero variance predictors
2. Center variables
3. Scale variables
4. Impute missing values using k-nearest neighbor

```{r}
message(sprintf("Number of complete cases before imputation = %d",
                complete.cases(dfTrain) %>% sum()))
nzv <- 
  dfTrain %>% 
  select(-c(id, Y1, Y2, Y3)) %>% 
  nearZeroVar(names = TRUE, saveMetric = TRUE) %>%
  mutate(varname = row.names(.)) %>% 
  filter(nzv == TRUE) %>% 
  select(varname, freqRatio, percentUnique, zeroVar, nzv) 
nzv %>% kable()
dfTrainPreProc1 <-
  dfTrain %>% 
  select(-one_of(nzv$varname)) %>% 
  select(-c(id, Y1, Y2, Y3))
preProc <-
  dfTrainPreProc1 %>% 
  preProcess(method = c("nzv", "corr", "knnImpute"), verbose = TRUE)
preProc
dfTrainPreProc2 <-  
  predict(preProc, dfTrainPreProc1) %>% 
  mutate(childAgeDichotomous = case_when(is.na(childAgeDichotomous) & childAge < 3 ~ 1,
                                         is.na(childAgeDichotomous) & childAge >= 3 ~ 2,
                                         TRUE ~ as.numeric(childAgeDichotomous))) %>% 
  mutate(childAgeDichotomous = factor(childAgeDichotomous, 
                                      levels = seq(2), 
                                      labels = c("Under 3", "3 or older")))
message(sprintf("Number of complete cases after imputation = %d",
                complete.cases(dfTrainPreProc) %>% sum()))
dfTrainPreProc <- dfTrainPreProc2
rm(dfTrainPreProc1, dfTrainPreProc2)
dfTrainPreProc %>% write.csv("data/processed/dfTrainPreProc.csv", row.names = FALSE)
```

## Training

Set the control parameters.

```{r}
ctrl <- trainControl(method = "repeatedcv",
                     number = 10,
                     repeats = 25,
                     savePredictions = TRUE,
                     allowParallel = TRUE,
                     search = "random")
```

Set the model and tuning parameter grid.

```{r}
library(randomForest)
method <- "rf"
grid <- expand.grid(mtry = seq(5, 10, 1))
```

Train model over the tuning parameters.

```{r trainingModelY1, fig.show = "hide"}
cl <- makeCluster(8)
registerDoParallel(cl)
trainingModel <-
  dfTrainPreProc %>% 
  select(-c(id, Y2, Y3)) %>% 
  train(Y1 ~ .,
        data = .,
        method = method,
        trControl = ctrl,
        tuneGrid = grid,
        tuneLength = 10,
        nthreads = 8,
        na.action = "na.omit",
        importance = TRUE)
stopCluster(cl)
trainingModel
trainingModel %>% 
  ggplot() + 
  geom_line() +
  geom_point() +
  theme_bw()
ggsave("figures/trainingModelY1.png")
ggsave("figures/trainingModelY1.svg")
save(trainingModel, file = "output/trainingModelY1.RData")
# load("output/trainingModelY1.RData")
```

![figures/trainingModelY1.png](figures/trainingModelY1.png)

```{r trainingModelY1-predict}
trainingModel$finalModel
varImp(trainingModel)
dfTrainPred <- 
  dfTrainPreProc %>% 
  mutate(hat = predict(trainingModel, dfTrainPreProc) %>% as.numeric())
cor(dfTrainPred %>% select(Y1, hat))
dfTrainPred %>% 
  ggplot() +
  ggtitle(sprintf("Correlation = %.03f", cor(dfTrainPred %>% select(Y1, hat)) %>% .[1, 2])) +
  aes(x = hat, y = Y1) +
  geom_abline(slope = 1, intercept = 0) +
  geom_smooth(method = "lm", formula = y ~ x - 1, se = FALSE) +
  geom_smooth(method = "lm", formula = y ~ x, color = "red", se = FALSE) +
  geom_point(alpha = 1/2)
```
